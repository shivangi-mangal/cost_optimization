# cost_optimization
Hybrid â€œFastâ€‘Archiveâ€ Architecture

API read/write     
   â†“
[ Cosmos DB container: â€œhot+warm dataâ€ (<=â€¯3â€¯months) ]
   â†“â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚              â”‚
      New writes always go          â”‚
      â†’ hot segment                 â”‚
                   â”‚              â”‚
    1. Archive job: move >3m data   â”‚
       â€¢ Extract old items using change feed
       â€¢ Store items in Azure Blob Storage (Cool tier, optionally Archive) as blobs/compressed JSON
       â€¢ Keep a stub/metadata document in Cosmos DB linking to blob location (or delete entire item)
    2. On read of stub: fetch blob from storage (Cool = subâ€‘second; Archive needs rehydration)
                   â”‚              â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
                                  â”‚
[ Blob Storage container: â€œarchival storeâ€ ]

Writes: unchanged â€” still go to Cosmos.

Reads:

1. If record in Cosmos (<=â€¯3â€¯m old): regular fast Cosmos query.

2. If record moved (stub exists): fetch blob via Storage SDK, decompress, returnâ€”milliseconds (cool tier).

3. No API client change â†’ all logic in service layer/code.

Cost Optimization Elements
1. Thin hot dataset + small Cosmos container(s)
Only recent 3â€‘month data stored, drastically smaller. Minimal RU cost, low storage overhead.

2. Archive old data to Azure Blob Storage (Cool or Archive)
Cool tier: milliseconds reads, low storage cost (~$0.01/GB) with modest transaction costs.
Archive tier: ultraâ€‘cheap (~$0.002/GB) but read latency up to 15â€¯hoursâ€”not ideal for your â€œsecondsâ€ SLA. So choose Cool tier unless access nearly zero. 

3. Use TTL or delete in Cosmos
After blob archiving, you can delete the full record in Cosmos or mark it with TTL. TTL autoâ€‘removes after a safe buffer, ensuring âˆ¼no dualâ€‘writes or downtime. 

4. Tune indexing policy
Large records (~300â€¯KB) bloat index size/RUs; exclude unqueried large fields (maybe the detailed payload) from indexing to reduce storage and RU cost.

 Implementation Plan
A. Backfill Archive Job (scheduled, e.g. Azure Function or Data Factory pipeline)
archive_job.py

B. Read Logic in API service:
api_layer.py 

C. TTL Cleanup Configuration:
Set TTL on container so stub removed automatically after buffer.

D. Indexing policy adjustment.

ğŸ›  No Downtime, No API Changes
1. API endpoints remain unchanged; internal routing handles archive vs live.
2. Migration runs in background; initial data pull then incremental (via change feed).
3. Cosmos always available; storage reads via Blob SDK are fast.

ğŸ“¦ Additional Optimizations
1. Autoscale or provisioned throughput: if serverless RU limits constrain, migrate to provisioned throughput with dynamic scaling via Azure SDK (no code change) to support growth. 
2. Compression: we gzip blobs to minimize blob storage cost.
3. Batch processing: incremental archive via Cosmos Change Feed instead of full scan.

ğŸ“… Cost Projections (Illustrative):
1. Cosmos: store only ~600â€¯GB Ã— 3â€¯m = ~150â€¯GB hot data â†’ ~USâ€¯$0.25/GB â†’ ~$37.5/month storage, plus much smaller RU cost for index & reads.
2. Blob Cool tier: ~450â€¯GB in Cool â†’ ~$4.5/month, plus low read cost.
3. Overall savings: >â€¯90â€¯% on storage cost, plus RU cost reduction.

ğŸ— Full Architecture Diagram (ASCII)
pgsql
Copy
Edit
Client API
    |
[Billing Service Layer]
    â”œâ”€ Query Cosmos â†’ if stub â†’ fetch blob from Azure Blob Storage
    â”œâ”€ Write Cosmos normally
    â””â”€ Archive Function job (on schedule / changeâ€‘feed):
           â†’ migrate old items to blob â†’ stub â†’ delete original â†’ TTL cleans stubs
